{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import watchdog\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import LoggingEventHandler\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "import os\n",
    "from os.path import join\n",
    "from enum import Enum, auto\n",
    "\n",
    "import wolff\n",
    "import wolff_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/s3182541/test.txt', '/Users/s3182541/test.npy', '/Users/s3182541/bye', '/Users/s3182541/subj_2_mem_neuron_data_first_0.npy', '/Users/s3182541/hi']\n",
      "Preparing sigma...\n",
      "Done with sigma.\n",
      "Done processing one data set of subject 2\n",
      "The original data files have been removed\n",
      "['/Users/s3182541/subj_2_initial_angles_0.npy']\n",
      "Done processing angles of subject 2\n",
      "The original data files have been removed\n"
     ]
    }
   ],
   "source": [
    "neuro_pat1 = re.compile(r\"neuron_data_cued\")\n",
    "neuro_pat2 = re.compile(r\"neuron_data_uncued\")\n",
    "angle_pat = re.compile(r\"initial_angles\")\n",
    "available_neuro1 = {}\n",
    "available_neuro2 = {}\n",
    "available_angle  = {}\n",
    "\n",
    "path = '/Users/s3182541'\n",
    "save_path = '/Users/s3182541/STSP/Decoding/data/final/exp1'\n",
    "\n",
    "NUM_PARTS = 1\n",
    "\n",
    "class FileType(Enum):\n",
    "    DATA   = auto()\n",
    "    ANGLES = auto()\n",
    "    \n",
    "# Group the thousands of neurons into a handful of channels \n",
    "def group(mem_data):\n",
    "    cut_data = mem_data[:, :250, :] # trials by 500 by neurons\n",
    "    num_channels = 17\n",
    "    neurons = np.mean(cut_data, 1).T # neurons by trials\n",
    "    kmeans = KMeans(n_clusters=num_channels, n_init=20, tol=1e-20).fit(neurons)\n",
    "    \n",
    "    data = np.empty((mem_data.shape[0], num_channels, mem_data.shape[1])) # trials by num_channels by timesteps\n",
    "    for channel in range(num_channels):\n",
    "        print(str(channel + 1) + \"/\" + str(num_channels), end='\\r')\n",
    "        data[:, channel, :] = np.mean(mem_data[:, :, kmeans.labels_ == channel], axis=2)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_files(available_dic, sub):\n",
    "    time.sleep(1)\n",
    "    save_dir = join(save_path, sub)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    \n",
    "    if available_dic == available_neuro1:\n",
    "        ft = FileType.DATA\n",
    "        save_file = join(save_dir, \"data1.npy\")\n",
    "        save_sigma = join(save_dir, \"sigma1.npy\")\n",
    "    elif available_dic == available_neuro2:\n",
    "        ft = FileType.DATA\n",
    "        save_file = join(save_dir, \"data2.npy\")\n",
    "        save_sigma = join(save_dir, \"sigma2.npy\")\n",
    "    else:\n",
    "        ft = FileType.ANGLES\n",
    "        save_file = join(save_dir, \"angles.npy\")\n",
    "        \n",
    "    parts = sorted(list(available_dic[sub]))\n",
    "    part = load_file(parts[0])\n",
    "    raw_data = np.empty((NUM_PARTS,) + part.shape, dtype=part.dtype)\n",
    "    raw_data[0] = part\n",
    "        \n",
    "    for i, file in enumerate(parts[1:]):\n",
    "        raw_data[i+1] = load_file(file)\n",
    "        \n",
    "    shape = list(part.shape)\n",
    "    shape.pop(0)\n",
    "    raw_data = raw_data.reshape([-1] + shape)\n",
    "        \n",
    "    if ft == FileType.ANGLES:\n",
    "        raw_data = raw_data / 180 * np.pi # Convert to radians\n",
    "        raw_data = raw_data * 2 # 'Scale' angles\n",
    "        np.save(save_file, raw_data)\n",
    "        \n",
    "        print(\"Done processing angles of subject \" + sub)\n",
    "    elif ft == FileType.DATA:\n",
    "        data = group(raw_data)\n",
    "        data += np.random.normal(scale=0.5, size=data.shape) # Prevent division by zero errors\n",
    "        np.save(save_file, data)\n",
    "\n",
    "        if __name__ == '__main__':\n",
    "            sigma = wolff_cross.prepare_sigma(data)\n",
    "\n",
    "        np.save(save_sigma, sigma)\n",
    "\n",
    "        print(\"Done processing one data set of subject \" + sub)\n",
    "        \n",
    "    \n",
    "    if os.access(path, os.W_OK | os.X_OK):\n",
    "        for file in parts:\n",
    "            os.remove(file)\n",
    "        print(\"The original data files have been removed\")\n",
    "    else:\n",
    "        print(\"No proper access to \" + path + \", files will not be removed\")\n",
    "\n",
    "def load_file(file):\n",
    "    while True:\n",
    "        try:\n",
    "            arr = np.load(file)\n",
    "            break\n",
    "        except (OSError, ValueError) as e:\n",
    "            print(str(e))\n",
    "            print(\"Error reading file, trying again...\")\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def get_dict(file):\n",
    "    if neuro_pat1.search(file):\n",
    "        return available_neuro1\n",
    "    elif neuro_pat2.search(file):\n",
    "        return available_neuro2\n",
    "    elif angle_pat.search(file):\n",
    "        return available_angle\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "class DataHandler(watchdog.events.FileSystemEventHandler):\n",
    "    def __init__(self):\n",
    "        super(DataHandler, self).__init__()\n",
    "        self.raw_files = []\n",
    "        self.processes = []\n",
    "    \n",
    "    def on_created(self, event):\n",
    "        if not event.is_directory:\n",
    "            self.raw_files.append(event.src_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processed_files = set()\n",
    "    processes = []\n",
    "    new_files = [join(path, f) for f in os.listdir(path) if os.path.isfile(join(path, f))]\n",
    "    \n",
    "    event_handler = DataHandler()\n",
    "    observer = Observer()\n",
    "    observer.schedule(event_handler, path, recursive=False)\n",
    "    observer.start()\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "\n",
    "            raw_files = event_handler.raw_files\n",
    "            new_files = new_files + [path for path in raw_files if path not in processed_files]\n",
    "\n",
    "            if new_files:\n",
    "                processed_files.update(new_files)\n",
    "                print(new_files)\n",
    "\n",
    "                for file in new_files:\n",
    "                    # The file name has the following syntax:\n",
    "                    # subj_[subject number]_[type of data]_[part_number].npy\n",
    "                    # where [type of data] may contain multiple underscores\n",
    "                    available_dic = get_dict(file)\n",
    "                    \n",
    "                    if available_dic is None:\n",
    "                        continue\n",
    "                    \n",
    "                    sub = file.split('_')[1]\n",
    "                    if sub not in available_dic:\n",
    "                        available_dic[sub] = set()\n",
    "\n",
    "                    available_dic[sub].add(file)\n",
    "                    \n",
    "                    # The maximum available number of parts is 6\n",
    "                    if len(available_dic[sub]) == NUM_PARTS:\n",
    "                        process_files(available_dic, sub)\n",
    "                        \n",
    "            new_files = []\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "        for p in event_handler.processes:\n",
    "            p.join()\n",
    "            \n",
    "    observer.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set1 = {1, 2}\n",
    "set2 = {2, 3}\n",
    "set2.difference(set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-10 15:51:59 - Created file: /Users/s3182541/test.npy\n",
      "2020-06-10 15:51:59 - Modified directory: /Users/s3182541\n",
      "2020-06-10 15:51:59 - Modified file: /Users/s3182541/test.npy\n",
      "2020-06-10 15:52:01 - Modified file: /Users/s3182541/test.npy\n",
      "2020-06-10 15:52:04 - Modified file: /Users/s3182541/test.npy\n",
      "2020-06-10 15:52:11 - Modified file: /Users/s3182541/test.npy\n",
      "2020-06-10 15:52:16 - Modified file: /Users/s3182541/test.npy\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format='%(asctime)s - %(message)s',\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    event_handler = LoggingEventHandler()\n",
    "    observer = Observer()\n",
    "    observer.schedule(event_handler, path, recursive=False)\n",
    "    observer.start()\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "    observer.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file3\n"
     ]
    }
   ],
   "source": [
    "myset = {'file1', 'file2', 'file3'}\n",
    "myset = sorted(list(myset))\n",
    "print(myset.pop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 3, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.empty((2, 3, 4))\n",
    "shape = list(arr.shape)\n",
    "new_arr = np.empty((6,) + arr.shape)\n",
    "shape.pop(0)\n",
    "new_arr.reshape([-1] + shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/share/Chiel4Loran/exp2/sim3'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname('/Users/share/Chiel4Loran/exp2/sim3/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.access(path, os.X_OK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
